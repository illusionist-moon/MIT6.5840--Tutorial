# Lab3 Raft

## 概述

6.5840（原6.824，之后统称6.5840）将lab3分为了4个part，分别是领导者选举(leader election)、日志(log)、持久化(persistence)、以及日志压缩(log compaction)，每一部分有各自的测试。能够理解，毕竟是要作为课程考核内容为学生进行打分的，能通过多少算多少。我们这些学习者在一开始上手去完成时，难免也会容易形成思维上的一个僵化——一个part一个part去完成就好。但事实上，这种想法会使得完成的过程非常艰辛，尤其是part4快照的加入，很可能会让没有一个对Raft有整体、宏观理解的学习者将他所完成的前三个part的代码全部重写。

在我开始学习6.5840之前，已经有了很多很好的教程，尤其是一位来自清华的学长的[文档](https://github.com/OneSizeFitsQuorum/MIT6.824-2021/blob/master/docs/lab2.md)，我在完成lab3时也很大程度借鉴了他的实现。但是，我还是感受到了很多的困难，有一些是更宏观的问题，有一些则是更corner的问题。这也是我所编写这个文档的目的——能够给同为Raft的学习者们在这两方面一些帮助。我才疏学浅，理解仅代表个人，如果有不足之处，还请包容。

在开始完成lab3前，一定要明白Raft究竟是做什么的、是如何运作的。首先Raft算法是一个共识(consensus)算法，共识算法的本质是解决分布式系统中多个节点之间对某个值达成一致意见的问题。再利用复制状态机(RSM)这一模型来保证分布式系统中多个节点间副本状态的一致。这里的副本，其实也就是paper中的状态机。而在lab4中，我们要实现一个基于Raft的KV数据库，状态机自然也就相应变成了KV Server中的data。要提到的是，它的运作流程其实6.5840有在lab4的部分给出，虽然连lab3都还没完成，但借助lab4中的应用例子，我认为反而能容易理解并完成lab3。lab4中的示意图如下所示：
![img.png](img/img-1.png)

client的请求首先打到KV Server上，Command可能是Get，也可能是Put。这里为了更好地说明，先以一个Put请求为例。首先，KV Server收到请求后，并不能直接去操纵自己的data，因为KV Server并不只自己这一台，需要保证client之后的请求打到其他KV Server时能够保证某种一致性【众所周知，Raft是能提供强一致性的共识算法。所谓强一致性，也被称为线性一致性、原子一致性。这种一致性模型是最强、最严格的，它意味着分布式系统中并发操作的结果与在单机上串行执行的结果是一样的，即需要所有操作能够获得一个全序关系】。因此，KV Server会将此次请求对应的Command传递给自己所对应的Raft节点，即lab3中的Start方法。当然，Raft算法的设计中，Command的处理应当由Raft中的leader节点作为入口。因此，在lab4中，当该KV Server所对应的Raft节点发现自己不是leader时，会通过Start方法的返回值告知KV Server，KV Server自然也会通过RPC的响应告知client。client则会切换KV Server重试。当Raft leader通过Start方法接收到了Command时，便会开始执行对这一个Command达成共识的过程。当超过半数的节点对

